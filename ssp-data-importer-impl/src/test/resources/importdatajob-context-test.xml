<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xmlns:batch="http://www.springframework.org/schema/batch"
       xmlns:p="http://www.springframework.org/schema/p"
       xsi:schemaLocation="
            http://www.springframework.org/schema/batch http://www.springframework.org/schema/batch/spring-batch-2.2.xsd
            http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd">

    <!-- step-scoped because I believe we want the resources expression to be re-evaluated on each execution. I.e.
        check for new files each time. -->
    <bean id="compositeRawCsvItemReader" class="org.springframework.batch.item.file.MultiResourceItemReader"
          scope="step"
          p:delegate-ref="singleRawCsvItemReader"
          p:resources="${batch.tables.input.folder}"> <!-- this would be parameterized to point at monitored path -->
          <!--p:resources="file://Users/dmccallum/dev1/ssp-data-import/src/ssp-data-import/ssp-data-import-impl/src/main/resources/prototype-input/*">-->

    </bean>

    <bean id="singleRawCsvItemReader" class="org.jasig.ssp.util.importer.job.csv.RawItemCsvReader" />

    <bean id="tableMetaDataRepository" class="org.jasig.ssp.util.importer.job.validation.map.metadata.database.JdbcTableColumnMetadataRepository">
        <constructor-arg name="dataSource" ref ="dataSource"/>
    </bean>

    <bean id="databaseConstraintRepository" class="org.jasig.ssp.util.importer.job.validation.map.metadata.database.CachingTableColumnMetadataRepository">
        <constructor-arg name="columnMetadataRepository" ref ="tableMetaDataRepository"/>
    </bean>
    <bean id="compositeFilteredCsvItemReader" class="org.springframework.batch.item.file.MultiResourceItemReader"
          scope="step"
          p:delegate-ref="singleFilteredCsvItemReader"
          p:resources="${batch.tables.input.folder}"> <!-- this would be parameterized to point at monitored path -->
          <!--p:resources="file://Users/dmccallum/dev1/ssp-data-import/src/ssp-data-import/ssp-data-import-impl/src/main/resources/prototype-input/*">-->

    </bean>
    
    <bean id="singleFilteredCsvItemReader" class="org.jasig.ssp.util.importer.job.staging.ProcessedItemCsvReader" />

        <!-- Internationalization (i18n) - Overwrites the default name of Spring. -->
    <bean id="messageSource" class="org.springframework.context.support.ResourceBundleMessageSource">
        <property name="basename" value="messages"/>
    </bean>


    <!-- Validations (also uses the messageSource defined above) -->
    <bean id="validator" class="org.springframework.validation.beanvalidation.LocalValidatorFactoryBean">
        <property name="validationMessageSource" ref="messageSource"/>
    </bean>

    <bean id="columnMetadataRepository" class="org.jarbframework.constraint.metadata.database.JdbcColumnMetadataRepository">
        <constructor-arg name="dataSource" ref ="dataSource"/>
    </bean>

    <bean id="metadataRepository" class="org.jasig.ssp.util.importer.job.config.MetadataConfigurations" scope="singleton"/>

    <bean id="rawItemValidateProcessor" class="org.jasig.ssp.util.importer.job.processor.RawItemValidateProcessor" >
        <property name="metadataRepository" ref="metadataRepository" />
    </bean>

    <bean id="rawItemEchoProcessor"
        class="org.jasig.ssp.util.importer.job.processor.RawItemEchoProcessor">
    </bean>

    <bean id="processedCsvItemProcessor"
      class="org.springframework.batch.item.support.CompositeItemProcessor">
        <property name="delegates">
             <list>
                 <ref local="processedItemValidateProcessor"/>
                 <ref local="processedItemEchoProcessor"/>
             </list>
       </property>
    </bean>
    
    <bean id="rawItemValidateProcessorListener"
      class="org.jasig.ssp.util.importer.job.listener.RawItemValidateProcessorListener">
    </bean>
    <bean id="processedItemValidateProcessor" class="org.jasig.ssp.util.importer.job.processor.ProcessedItemValidateProcessor" >
    </bean>

    <bean id="processedItemEchoProcessor"
        class="org.jasig.ssp.util.importer.job.processor.ProcessedItemEchoProcessor">
    </bean>

    <bean id="rawCsvItemProcessor"
      class="org.springframework.batch.item.support.CompositeItemProcessor">
        <property name="delegates">
             <list>
                 <ref local="rawItemValidateProcessor"/>
                 <ref local="rawItemEchoProcessor"/>
             </list>
       </property>
    </bean>
    <bean id="processedCsvItemWriter" class="org.jasig.ssp.util.importer.job.csv.RawItemCsvWriter"
          scope="step"
          p:preamble="${exists.only.for.testing.1}" /> <!-- Dummy impl needs to be scoped to the step so it can reset itself on execution -->
    

	
<!-- 
    <bean id="stagingTablesItemWriter" class="org.jasig.ssp.util.importer.job.csv.RawItemCsvWriter"
          scope="step"
          p:preamble="${exists.only.for.testing.1}" /> Dummy impl needs to be scoped to the step so it can reset itself on execution -->


    <!-- TODO these are both just dummy impls for the prototype -->
    <bean id="partialUploadGuard" class="org.jasig.ssp.util.importer.job.tasklet.PartialUploadGuard" />
    <bean id="batchInitializer" class="org.jasig.ssp.util.importer.job.tasklet.BatchInitializer"/>
    <bean id="stagingTableTruncator" class="org.jasig.ssp.util.importer.job.listener.StagingTableTruncator"
   		  p:dataSource-ref="dataSource">
        <property name="metadataRepository" ref="metadataRepository" />
    </bean>
    
    <batch:job id="importJob" restartable="false">

        <!-- === Start Core Job Step Definition == -->
        <!-- Make sure we don't accidentally process a partial upload -->
        <batch:step id="guardPartialUploads" next="initializeBatch">
            <!-- Probably just needs to be a TaskletStep rather than a chunk b/c there's not really read/process/writer
            workflow involved -->
            <batch:tasklet ref="partialUploadGuard"></batch:tasklet>
        </batch:step>

        <!-- Another step might be need here if we have to guard against concurrent executions (I think the framework
        takes care of that for us, esp if we're using a db-backed JobRepository.) -->
        <!--<batch:step id="guardConcurrentExecution">-->
        <!--</batch:step>-->

        <!-- Make sure we have a clean working directory, copy new files into it. -->
        <batch:step id="initializeBatch" next="filterRawItems">
            <!-- Probably just needs to be a TaskletStep rather than a chunk b/c there's not really read/process/writer
            workflow involved -->
            <batch:tasklet ref="batchInitializer"></batch:tasklet>
        </batch:step>

        <!-- Read raw files, map them to validatable beans, validate those beans, write valid records back out
        to another set of files -->
        <batch:step id="filterRawItems" next="stageFilteredItems">  <!-- stop here for prototype -->
            <batch:tasklet>
                <!-- Can add a @skip-limit we can control how many exceptional skips we'll allow before abandoning the
                step. Note that this limit applies separately to reads, processes, and writes. -->
                <batch:chunk reader="compositeRawCsvItemReader" processor="rawCsvItemProcessor" writer="processedCsvItemWriter"
                             commit-interval="50" skip-limit="1">

                    <!-- List out exceptions which should result in a record simply being skipped, up to some
                    configurable limit set on the chunk above -->
                    <batch:skippable-exception-classes>
                        <batch:include class="java.lang.Exception"/>
                        <batch:include class="org.postgresql.util.PSQLException"/>
                        <batch:include class="org.springframework.beans.factory.BeanCreationException"/>
                    </batch:skippable-exception-classes>

                    <!-- Step-, chunk-, item-read, item-process, item-write, skip-scoped listeners, if needed. -->
                    <batch:listeners>
                        <batch:listener ref="rawItemValidateProcessorListener" />
                    </batch:listeners>
                </batch:chunk>
            </batch:tasklet>
        </batch:step>


        <!-- Read validated files, write output to staging tables -->
        <batch:step id="stageFilteredItems" > <!--  next="writeFilteredItems"> -->
        
  
            <batch:tasklet>
                <!-- Can add a @skip-limit we can control how many exceptional skips we'll allow before abandoning the
                step. Note that this limit applies separately to reads, processes, and writes. -->
                <batch:chunk reader="compositeFilteredCsvItemReader" processor="processedCsvItemProcessor" writer="stagingTableItemWriter"
                             commit-interval="10" skip-limit="100">

                    <!-- List out exceptions which should result in a record simply being skipped, up to some
                    configurable limit set on the chunk above -->
                    <batch:skippable-exception-classes>
                        <batch:include class="java.lang.Exception"/>
                        <!--<exclude class="java.io.FileNotFoundException"/>-->
                    </batch:skippable-exception-classes>

                    <!-- Step-, chunk-, item-read, item-process, item-write, skip-scoped listeners, if needed. -->
		   		     <batch:listeners>
					     <batch:listener ref="stagingTableTruncator" />
				     </batch:listeners> 
		         </batch:chunk>
                
            </batch:tasklet>
              
        </batch:step>
        <!-- === End Core Job Step Definition == -->


        <!-- Use this for custom JobParameter validation, if necessary. Note that you get some simple validation for
        free by using or extending DefaultJobParametersValidator -->
        <!--<batch:validator ref="importJobValidator" />-->


        <!-- Watch for beforeJob() and afterJob() events here. Note that afterJob() is always fired unless
         the entire job crashes catastrophically. I.e. you can expect to handle job-level failures and successes
         in that event. -->
        <!--<batch:listeners></batch:listeners>-->
    </batch:job>

</beans>

